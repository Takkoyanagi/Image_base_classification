{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network- Classification of Cat vs. Dog Using Keras and Tensorflow backend\n",
    "### Project from Machine Learning A-Z<sup>TM</sup> by Kirill Eremenko and Hadelin De Ponteves\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first begin by importing the necessary libraries and packages for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for initialization of CNN\n",
    "## 1) Convolution operation/ ReLU layer-rectifier (reduce non-linearity)\n",
    "## 2) Pooling of data/down sampling to reduce the size, parameters, preserved features, account for textual/spacial invaraince, minimize overfitting\n",
    "## 3) Adding additional convolutional layers\n",
    "## 4) Flattening of the data - becomes the input layer to NN\n",
    "## 5) Full connection of the nuero networks\n",
    "## 6) Completion of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\takko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Adding a second convolutional layer\n",
    "classifier.add(Conv2D(64, 3, 3, activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 4 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 5 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Step 6 - Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN to the scaled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "C:\\Users\\takko\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=250, validation_data=<keras_pre..., validation_steps=50, epochs=25)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 85s 338ms/step - loss: 0.6664 - acc: 0.5945 - val_loss: 0.6080 - val_acc: 0.6663\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 110s 439ms/step - loss: 0.5927 - acc: 0.6866 - val_loss: 0.5545 - val_acc: 0.7317\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 106s 426ms/step - loss: 0.5371 - acc: 0.7289 - val_loss: 0.5512 - val_acc: 0.7273\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 108s 431ms/step - loss: 0.5152 - acc: 0.7419 - val_loss: 0.4776 - val_acc: 0.7734\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 109s 436ms/step - loss: 0.4882 - acc: 0.7658 - val_loss: 0.4854 - val_acc: 0.7694\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 104s 417ms/step - loss: 0.4641 - acc: 0.7799 - val_loss: 0.4708 - val_acc: 0.7784\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 111s 444ms/step - loss: 0.4556 - acc: 0.7817 - val_loss: 0.4698 - val_acc: 0.7759\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 107s 426ms/step - loss: 0.4374 - acc: 0.7877 - val_loss: 0.4644 - val_acc: 0.7816\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 98s 391ms/step - loss: 0.4166 - acc: 0.8040 - val_loss: 0.4794 - val_acc: 0.7835\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 108s 434ms/step - loss: 0.3985 - acc: 0.8190 - val_loss: 0.4618 - val_acc: 0.7856\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 106s 424ms/step - loss: 0.3837 - acc: 0.8224 - val_loss: 0.4469 - val_acc: 0.8074\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 107s 429ms/step - loss: 0.3682 - acc: 0.8345 - val_loss: 0.4502 - val_acc: 0.8138\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 99s 397ms/step - loss: 0.3574 - acc: 0.8399 - val_loss: 0.4715 - val_acc: 0.7879\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 102s 408ms/step - loss: 0.3466 - acc: 0.8449 - val_loss: 0.4684 - val_acc: 0.7986\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 113s 451ms/step - loss: 0.3264 - acc: 0.8541 - val_loss: 0.4903 - val_acc: 0.8050\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 114s 456ms/step - loss: 0.3095 - acc: 0.8668 - val_loss: 0.4578 - val_acc: 0.8226\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 111s 442ms/step - loss: 0.2915 - acc: 0.8781 - val_loss: 0.4360 - val_acc: 0.8138\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 110s 440ms/step - loss: 0.2794 - acc: 0.8818 - val_loss: 0.4898 - val_acc: 0.8112\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 112s 448ms/step - loss: 0.2524 - acc: 0.8909 - val_loss: 0.5407 - val_acc: 0.7936\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 94s 376ms/step - loss: 0.2426 - acc: 0.9001 - val_loss: 0.5862 - val_acc: 0.8013\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.2347 - acc: 0.9044 - val_loss: 0.4726 - val_acc: 0.8037\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 83s 334ms/step - loss: 0.2220 - acc: 0.9074 - val_loss: 0.5321 - val_acc: 0.7967\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 84s 336ms/step - loss: 0.2109 - acc: 0.9125 - val_loss: 0.5160 - val_acc: 0.8100\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 86s 344ms/step - loss: 0.2057 - acc: 0.9164 - val_loss: 0.6585 - val_acc: 0.7917\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 94s 375ms/step - loss: 0.1758 - acc: 0.9326 - val_loss: 0.6066 - val_acc: 0.8037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x183ad06b748>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images and avoid overfitting by using image augmentation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 250,\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: Running the model took ~43 minutes with a classification accuracy of 93% and validation accuracy of 80%. The overfitting problem observed may be mitigated by increasing the number of images.\n",
    "\n",
    "# Further improvements for the model can be accomplished by adding more convolutional layers, increase hidden layers, and increaseing the resolution of the image file from 64 by 64  to a higher resolution to gain more features for the model to find. However, we should process the data using a gpu instead of a the currently used cpu to allow for practical computational speeds/time.\n",
    "\n",
    "### Thank you for viewing my project - Tak Koyanagi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
